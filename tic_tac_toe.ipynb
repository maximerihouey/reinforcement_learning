{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(p1, p2, env, draw=False):\n",
    "    current_player = None\n",
    "    \n",
    "    # loops until the game is over\n",
    "    while not env.game_over():\n",
    "        # alternate between players (p1 always first)\n",
    "        if current_player == p1:\n",
    "            current_player = p2\n",
    "        else:\n",
    "            current_player = p1\n",
    "        \n",
    "        # make a move\n",
    "        current_player.take_action(env)\n",
    "        \n",
    "        # draw if necessary\n",
    "        if draw:\n",
    "            if draw == 1 and current_player == p1:\n",
    "                env.draw_board()\n",
    "            if draw == 2 and current_player == p2:\n",
    "                env.draw_board()\n",
    "        \n",
    "        # update histories\n",
    "        state = env.get_state()\n",
    "        p1.update_state_history(state)\n",
    "        p2.update_state_history(state)\n",
    "    \n",
    "    if draw:\n",
    "        env.draw_board()\n",
    "    \n",
    "    # value function update\n",
    "    p1.update(env)\n",
    "    p2.update(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LENGTH = 3\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((LENGTH, LENGTH))\n",
    "        self.x = -1 # x on the board, player 1\n",
    "        self.o = 1 # o on the board, player 2\n",
    "        self.winner = None\n",
    "        self.ended = False\n",
    "        self.num_states = 3 ** (LENGTH*LENGTH)\n",
    "    \n",
    "    def is_empty(self, i, j):\n",
    "        return self.board[i,j] == 0\n",
    "        \n",
    "    def reward(self, symbol):\n",
    "        # no reward until game is over\n",
    "        if not self.game_over():\n",
    "            return 0\n",
    "        \n",
    "        # game is over, symbol is either self.x or self.o\n",
    "        if self.winner == symbol:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_state(self):\n",
    "        # retruens the current state, represented as an int\n",
    "        # S = set of all possibles states\n",
    "        # |S| = 3 ^ (BOARD SIZE) [each cell has 3 possible values]\n",
    "        # some satte are not possible\n",
    "        # trinanry to decimal implementation\n",
    "        k = 0\n",
    "        h = 0\n",
    "        for i in xrange(LENGTH):\n",
    "            for j in xrange(LENGTH):\n",
    "                if self.board[i,j] == 0:\n",
    "                    v = 0\n",
    "                elif self.board[i,j] == self.x:\n",
    "                    v = 1\n",
    "                elif self.board[i,j] == self.o:\n",
    "                    v = 2\n",
    "                h += (3**k) * v\n",
    "                k += 1\n",
    "        return h\n",
    "    \n",
    "    def game_over(self, force_recalculate=False):\n",
    "        if not force_recalculate and self.ended:\n",
    "            return self.ended\n",
    "        \n",
    "        # check rows\n",
    "        for i in xrange(LENGTH):\n",
    "            for play in (self.x, self.o):\n",
    "                if self.board[i].sum() == player*LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "        # check columns\n",
    "        for j in xrange(LENGTH):\n",
    "            for play in (self.x, self.o):\n",
    "                if self.board[:,j].sum() == player*LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "        # check diagonals\n",
    "        for player in (self.x, self.o):\n",
    "            # top-left -> bottom-right\n",
    "            if self.board.trace() == player*LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "            # top-right -> bottom-left\n",
    "            if np.flipr(self.board).trace() == player*LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "        # check if draw\n",
    "        if np.all((self.board == 0) == False):\n",
    "            # winner stays None\n",
    "            self.winner = None\n",
    "            self.ended = True\n",
    "            return True\n",
    "        # game is not over\n",
    "        self.winner = None\n",
    "        return False\n",
    "    \n",
    "    def draw_board(self):\n",
    "        for i in xrange(LENGTH):\n",
    "            print \"------------\"\n",
    "            for j in xrange(LENGTH):\n",
    "                print \" \",\n",
    "                if self.board[i,j] == self.x:\n",
    "                    print \"x\",\n",
    "                elif self.board[i,j] == self.o:\n",
    "                    print \"o\",\n",
    "                else:\n",
    "                    print \" \",\n",
    "            print \"\"\n",
    "        print \"------------\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, eps=0.1, alpha=0.5):\n",
    "        self.eps = eps # probability of choosing random action instead of greedy\n",
    "        self.alpha = alpha\n",
    "        self.verbose = False\n",
    "        self.state_history = ()\n",
    "        \n",
    "    def setV(self, V):\n",
    "        self.V = V\n",
    "        \n",
    "    def set_symbol(self, symbol):\n",
    "        self.sym = sym\n",
    "    \n",
    "    def set_verbose(self, v):\n",
    "        # if true will print value for each position on the board\n",
    "        self.verbose = v\n",
    "        \n",
    "    def reset_history(self):\n",
    "        self.state_history = []\n",
    "        \n",
    "    def take_action(self, env):\n",
    "        # choose an action based on epsilon-greedy strategy\n",
    "        r = np.random.rand()\n",
    "        best_state = None\n",
    "        if r < self.eps:\n",
    "            # take a random action\n",
    "            if self.verbose:\n",
    "                print \"Taking a random action\"\n",
    "            \n",
    "            possible_moves = []\n",
    "            for i in xrange(LENGTH):\n",
    "                for j in xrange(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                        possible_moves.append((i, j))\n",
    "            idx = np.random.choice(len(possible_moves))\n",
    "            next_move = possible_moves[idx]\n",
    "        else:\n",
    "            pos2value = {}\n",
    "            next_move = None\n",
    "            best_value = -1\n",
    "            for i in xrange(LENGTH):\n",
    "                for j in xrange(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                        # what is the state if we made this move ?\n",
    "                        env.board[i, j] = self.sym\n",
    "                        state = env.get_state()\n",
    "                        env.board[i, j] = 0 # don't forget to change it back !\n",
    "                        pos2value[(i, j)] = self.V[state]\n",
    "                        if self.V[state] > best_value:\n",
    "                            best_value = self.V[state]\n",
    "                            best_state = state\n",
    "                            next_move = (i, j)\n",
    "            # if verbose, draw the board w/ the values\n",
    "            if self.verbose:\n",
    "                print \"Taking a greedy action\"\n",
    "                for i in xrange(LENGTH):\n",
    "                    print \"----------------\"\n",
    "                    for j in xrange(LENGTH):\n",
    "                        if env.is_empty(i, j):\n",
    "                            # print the value\n",
    "                            print \"%.2f|\" % pos2value[(i, j)],\n",
    "                        else:\n",
    "                            print \" \",\n",
    "                            if env.board[i, j] == env.x:\n",
    "                                print \"x |\",\n",
    "                            elif env.board[i, j] == env.o:\n",
    "                                print \"o |\",\n",
    "                            else:\n",
    "                                print \" |\",\n",
    "                    print \"\"\n",
    "                print \"----------------\"\n",
    "    \n",
    "    def update_state_history(self, s):\n",
    "        # cannot put this in take_action, because take_action only happens once\n",
    "        # every other iteration for each player\n",
    "        # state history needs to be updated every iteration\n",
    "        # s = env.get_state() # don't want to do this twice so pass it in\n",
    "        self.state_history.append(s)\n",
    "        \n",
    "    def update(self, env):\n",
    "        # we want to BACKTRACK over the states, so that:\n",
    "        # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
    "        # where V(next_state) = reward if it's the most current_state\n",
    "        #\n",
    "        # NOTE: we only do this at the end of an episode\n",
    "        # not so for all the algorithms we will study\n",
    "        reward = env.reward(self.sym)\n",
    "        target = reward\n",
    "        for prev in reversed(self.state_history):\n",
    "            value = self.V[prev] + self.alpha * (target - self.V[prev])\n",
    "            self.V[prev] = value\n",
    "            target = value\n",
    "        self.reset_history()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "    \n",
    "    def take_action(self, env):\n",
    "        while True:\n",
    "            # break if we make a legal mov\n",
    "            move = raw_input(\"Enter coordinates i,j for your next move (i, j=0..2): \")\n",
    "            i, j = move.split(\",\")\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            if env.is_empty(i, j):\n",
    "                env.board[i, j] = self.sym\n",
    "                break\n",
    "    \n",
    "    def update(self, env):\n",
    "        pass\n",
    "    \n",
    "    def update_state_history(self, s):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_state_hash_and_winner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-46fc2ff45cca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set initial V for p1 and p2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstate_winner_triples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state_hash_and_winner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mVx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialV_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_winner_triples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_state_hash_and_winner' is not defined"
     ]
    }
   ],
   "source": [
    "# players (agents)\n",
    "p1 = Agent()\n",
    "p2 = Agent()\n",
    "\n",
    "# set initial V for p1 and p2\n",
    "env = Environment()\n",
    "state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "Vx = initialV_x(env, state_winner_triples)\n",
    "p1.setV(Vx)\n",
    "Vo = initialV_o(env, state_winner_triples)\n",
    "p2.setV(Vo)\n",
    "\n",
    "# give each player their symbol\n",
    "p1.set_symbol(env.x)\n",
    "p2.set_symbol(env.o)\n",
    "\n",
    "\n",
    "# playin !\n",
    "T = 10\n",
    "for t in xrange(T):\n",
    "    if t % 10 == 0:\n",
    "        print t\n",
    "    play_game(p1, p2, Environment())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing against the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'player' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-df5976f2ae69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# making the AI player 1 allow to see which starting moves it takes (center ?)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-12acd4e85c52>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(p1, p2, env, draw)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# loops until the game is over\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# alternate between players (p1 always first)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_player\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3df292c9b881>\u001b[0m in \u001b[0;36mgame_over\u001b[0;34m(self, force_recalculate)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mplay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mLENGTH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwinner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'player' referenced before assignment"
     ]
    }
   ],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.o)\n",
    "\n",
    "while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(p1, human, Environment(), draw=2)\n",
    "    # making the AI player 1 allow to see which starting moves it takes (center ?)\n",
    "    answer = raw_input(\"Play again ? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] = 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
